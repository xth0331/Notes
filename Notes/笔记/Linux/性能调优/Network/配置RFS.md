# 配置 RFS

RFS（接收端流的控制）扩展了 RPS 的性能以增加 CPU 缓存命中率，以此减少网络延迟。RPS 仅基于队列长度转发数据包，RFS 使用 RPS 后端预测最合适的 CPU，之后会根据应用程序处理数据的位置来转发数据包。这增加了 CPU 的缓存效率。

RFS 是默认禁用的。要启用 RFS，用户须编辑两个文件：

- `/proc/sys/net/core/rps_sock_flow_entries`

  设置此文件至同时活跃连接数的最大预期值。对于中等服务器负载，推荐值为 `32768` 。所有输入的值四舍五入至最接近的2的幂。

- `/sys/class/net/device/queues/rx-queue/rps_flow_cnt`

  将 *device* 改为想要配置的网络设备名称（例如，`eth0`），将 *rx-queue* 改为想要配置的接收队列名称（例如，`rx-0`）。将此文件的值设为 `rps_sock_flow_entries` 除以 `N`，其中 `N` 是设备中接收队列的数量。例如，如果 `rps_flow_entries` 设为 `32768`，并且有 16 个配置接收队列，那么 `rps_flow_cnt` 就应设为 `2048`。对于单一队列的设备，`rps_flow_cnt` 的值和 `rps_sock_flow_entries` 的值是一样的。

从单个发送程序接收的数据不会发送至多个 CPU。如果从单个发送程序接收的数据多过单个 CPU 可以处理的数量，须配置更大的帧数以减少中断数量，并以此减少 CPU 的处理工作量。或是考虑 NIC 卸载选项来获得更快的 CPU。

考虑使用 `numactl` 或 `taskset` 与 RFS 相结合，以将应用程序固定至特定的内核、 socket 或 NUMA 节点。这可以有助于防止数据处理紊乱。

## 配置加速 RFS

加速 RFS 是通过添加硬件协助来增速的。如同 RFS，数据转发是基于应用程序处理数据包的位置。但不同于传统 RFS 的是，数据是直接发送至处理数据线程的本地 CPU：即运行应用程序的 CPU，或是对于在缓存层次结构中的 CPU 来说的一个本地 CPU。

加速 RFS 只有满足以下条件才可以使用：

- 网络接口卡须支持加速 RFS。加速 RFS 是由输出 `ndo_rx_flow_steer()` `netdevice` 功能的接口卡支持。
- `ntuple` 筛选必须启用。

一旦满足了这些条件，队列映射 CPU 就会基于传统 RFS 配置自动导出。即队列映射 CPU 会基于由每个接收队列的驱动程序配置的 IRQ 关联而自动导出。

推荐在可以使用 RFS 以及网络接口卡支持硬件加速时使用加速 RFS 。