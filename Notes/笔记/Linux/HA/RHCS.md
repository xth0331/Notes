# 部署RHCS

## 基础环境信息

描述本次基础环境信息：

![](https://blog-image.nos-eastchina1.126.net/9159.png)







### 系统信息

| 操作系统版本                                  | 操作系统镜像                     |
| --------------------------------------------- | -------------------------------- |
| `Red Hat Enterprise Linux Server release 6.8` | `rhel-server-6.8-x86_64-dvd.iso` |

### 

### 主机信息

| 主机名       | 主机IP地址        | 角色            |
| ------------ | ----------------- | --------------- |
| `fence`      | x.x.x.x           | 仿真fence设备   |
| `RHCE-node1` | `192.168.220.221` | RHCS集群节点-01 |
| `RHEL-node2` | `192.168.220.222` | RHCS集群节点-02 |

### RHCS中启用的IP端口

| IP 端口号  | 协议 | 组件                          |
| :--------- | :--- | :---------------------------- |
| 5404, 5405 | UDP  | `corosync/cman`（集群管理器） |
| 11111      | TCP  | `ricci`（推广更新的集群信息） |
| 21064      | TCP  | `dlm`（发布的锁定管理器）     |
| 16851      | TCP  | `modclusterd`                 |

### 存储信息

| 存储类型 | 多路径软件                |
| -------- | ------------------------- |
| `iSCSI`  | `device-mapper-multipath` |

**iSCSI只需要配置`initiator`与`target`可以请求即可，实际环境若为FC-SAN，请将主机的wwpn提供给存储管理员。**

**查看wwpn的方法**:`cat /sys/class/fc_host/host*/port_name`



## 注意事项

可使用各种方法配置红帽高可用性附加组件以满足您的需要。当进行计划、配置和实施的部署时，请考虑以下常规注意事项：

- 支持的集群节点数

  红帽高可用性附加组件最多支持的集群节点数为 16。

- 单点集群

  现在只能完全支持单点集群。

- GFS2

  虽然 GFS2 文件系统既可作为独立系统使用，也可作为集群配置的一部分，但不支持将 GFS2 作为单节点文件系统使用。红帽支持很多为单节点优化的高性能单节点文件系统，它们相对集群文件系统来说支出更低。红帽将继续为现有客户支持单节点 GFS2 文件系统。当您将 GFS2 文件系统作为集群文件系统配置时，必须确定该集群中的所有节点都可访问共享的文件系统。不支持不对称集群配置，在不对称集群中，有些节点可访问该文件系统，而其他节点则不能。这不要求所有节点确实挂载该 GFS2 文件系统。

- 无单点故障硬件配置

  集群可包括一个双控制器 RAID 阵列、多绑定链路、集群成员和存储间的多路径以及冗余无间断供电（UPS）系统以保证没有单点故障造成的应用程序失败或者数据丢失。另外，可设置一个低消耗集群以提供比无单点故障集群低的可用性。例如：您可以设置一个使用单控制器 RAID 阵列和只使用单以太网链路的集群。某些低消耗备选方案，比如主机 RAID 控制器、无集群支持的软件 RAID 以及多启动器平行 SCSI 配置与共享集群存储不兼容，或者不适合作为共享集群存储使用。

- 确保数据完整

  要保证数据完整，则每次只能有一个节点可运行集群服务和访问集群服务数据。在集群硬件配置中使用电源开关，就可让一个节点在故障切换过程中，重启节点 HA 服务前为另一个节点提供动力。这样就可防止两个节点同时访问同一数据并破坏数据。强烈建议使用 *Fence 设备*（远程供电、关闭和重启集群节点的硬件或者软件解决方案），以确保在所有失败情况下数据的完整性。

- 以太网通道绑定

  集群仲裁以及节点是否正常运行是由在通过以太网在集群节点间的沟通信息确定的。另外，集群节点使用以太网执行各种重要集群功能（例如：fencing）。使用以太网通道绑定，可将多个以太网接口配置为作为一个接口动作，这样就减小了在集群节点间以及其他集群硬件间典型切换的以太网连接单点故障风险。

- IPv4 和 IPv6

  高可用性附加组件支持 IPv4 和 IPv6 互联网协议。

##  环境准备

### 关闭防火墙

*暂时关闭，如果需要细化防火墙规则，文档最后会列出详细的防火墙配置*

```bash
iptables -F 		# 清空iptables规则（如果默认策略是Drop，请不要执行此条命令。）
iptables-save   # 保存配置
iptables -L -n    # 查看当前规则
```

### 禁用或删除NetworkManager

*不支持在集群节点中使用 `NetworkManager`。如果您已经在集群节点中安装了 `NetworkManager`，您应该删除或者禁用该程序。*

直接删除：

```bash
yum remove -y NetworkManager  # 移除包
```

或者不删除,考虑禁用：

```bash
service NetworkManager stop #关闭服务
chkconfig NetworkManager off # 取消开机启动
```

### 关闭SELinux

如果熟悉的话可以配置，不熟悉的话，就直接关闭吧。

```bash
setenforce 0
sed -i s/SELINUX=enforcing/SELINUX=disabled/g  /etc/selinux/config     
reboot
```

**彻底关闭SELinux需要重启，如果暂时不方便重启，可以先执行`setenforce 0`,等方便的时候重启服务器。**



### 配置本地Yum源

在`/etc/yum.repos.d`目录下创建新的本地`yumrepo`文件，本示例的`repo`文件名为`redhat-base.repo`。

```bash
mount /dev/sr0 /mnt
vi  /etc/yum.repos.d/redhat-base.repo
```

以下是文件内容，*(假设ISO文件挂载至`/mnt`下)*。

```bash
[base]
name=base
baseurl=file:///mnt
enabled=1
gpgcheck=0

[HighAvailability]
name=HighAvailablity
baseurl=file:///mnt/HighAvailability
enabled=1
gpgcheck=0

[ResilientStorage]
name=ResilientStorage
baseurl=file:///mnt/ResilientStorage
enabled=1
gpgcheck=0

[LoadBalancer]
name=LoadBalancer
baseurl=file:///mnt/LoadBalancer
enabled=1
gpgcheck=0
```

配置保存完成后，执行如下命令：

```bash
yum clean all 
yum makecache
```



### 添加存储

1. 安装`iscsi-initiator-utils`与`device-mapper-multipath`

   ```bash
   yum install -y iscsi-initiator-utils device-mapper-multipath
   ```

2. 发现`iscsi`target

   ```bash
   iscsiadm -m discovery -t sendtargets -p <targetIP1:PORT>	# 发现target
   iscsiadm -m discovery -t sendtargets -p <targetIP2:PORT> 	# 发现target
   chkconfig iscsi on   # 开机启动iscsi服务
   iscsiadm -m node -T <IQN1> -p <IP1:PORT> --login				# 登录
   iscsiadm -m node -T <IQN2> -p <IP2:PORT> --login				# 登录
   iscsiadm -m node -T <IQN1> -p <IP1:PORT> -op update -n node.startup -v automatic	# 自动登录
   iscsiadm -m node -T <IQN2> -p <IP2:PORT> -op update -n node.startup -v automatic	# 自动登录
   ```

3. 生成`/etc/multipath.conf`

   ```bash
   multipath --enable --with_multipathd y
   ```

4. 启动`multipathd`服务

   ```bash
   chkconfig multipathd on  			# 开机启动
   chkconfig --list multipathd 		# 检查
   service multipathd restart			# 重启服务
   service multipathd status 			# 检查
   ```

5. 查看多路径设

   ```bash
   multipath -ll  # 查看路径
   lsblk 				  # 查看设备
   ```



### 时间同步

ntp配置略



## 安装luci

1. 在节点1上安装`luci`

```bash
yum -y install luci
```

## 安装ricci,cman和rgmanager

1. 在两个RHCS节点上安装：

   ```bash
   yum -y install ricci cman rgmanager 
   ```

2. 配置`ricci`密码：

   ```bash
   passwd ricci
   ```



## 启动luci

使用`luci`配置集群要求在集群中安装并运行`ricci` 

1. 启动`ricci`服务：

     ```bash
      service ricci start  # 启动服务
      chkconfig ricci on # 开机启动
     ```

2. 启动`luci`服务：

   ```bash
   service luci start # 启动服务
   chkconfig luci on # 开机启动
   ```

3. 通过浏览器访问`https://LUCI-IP:8084`来访问luci，当启动luci服务时，登录`url`会回显到标准输出。

**注意**

初始情况下，只能通过root的身份验证信息访问；

如果 15 分钟后没有互动，则 **luci** 会处于闲置超时而让您退出。

 

## 创建集群

使用 **luci** 创建集群包括命名集群、在集群中添加集群节点、为每个节点输入 **ricci** 密码并提交创建集群请求。如果节点信息和密码正确，则 **Conga** 会自动在集群节点中安装软件（如果当前没有安装适当的软件包）并启动集群。按如下步骤创建集群：

1. 在 **luci** **「Homebase」**页面左侧菜单中点击**「管理集群」**。此时会出现**「集群」**页面。

   ![](https://blog-image.nos-eastchina1.126.net/luci-01.png)

   

2. 点击**「创建」**后出现**「创建集群页面」**。

   ![](https://blog-image.nos-eastchina1.126.net/luci-03.png)

   **图 3.3. 创建 luci 集群对话框**

3. 请根据需要在**「创建新集群」**页面中输入以下参数：

   - 在**「集群名称」**文本框中输入集群名称。集群名称不能超过 15 个字符。

   - 如果集群中的每个节点都有同样的 **ricci** 密码，您可以选择**「在所有节点中使用相同的密码」**，这样就可在您添加的节点中自动填写**「密码」**字段。

   - 在**「节点名称」**栏中输入集群中节点的名称，并在**「密码」**栏中为该节点输入 **ricci**密码。

   - 如果为您的系统配置了专门用于集群流量的专门的私有网络，则最好将 **luci** 配置为使用与集群节点名称解析拨通的地址与 **ricci** 进行沟通。您可以在**「Ricci 主机名」**中输入该地址达到此目的。

   - 如果您要在 **ricci** 代理中使用不同的端口，而不是默认的 11111 端口，您可以更改那个参数。

   - 点击**「添加另一个节点」**并输入节点名称，同时为集群的每个附加节点输入 **ricci** 密码。

   - 如果您不想要在创建集群时升级已经在节点中安装的集群软件软件包，请选择**「使用本地安装的软件包」**选项。如果您要升级所有集群软件软件包，请选择**「下载软件包」**选项。

     **注意**

     如果缺少任意基本集群组件（`cman`、`rgmanager`、`modcluster` 及其所有相依性软件包），无论是选择**「使用本地安装的软件包」**，还是**「下载软件包」**选项，都会安装它们。如果没有安装它们，则创建节点会失败。

   - 需要时选择**「加入集群前重启节点」**。

   - 如果需要集群的存储，则请选择**「启动共享存储支持」**。这样做将下载支持集群存储的软件包，并启用集群的 LVM。您应该只能在可访问弹性存储附加组件或者可扩展文件系统附加组件时选择这个选项。

4. 点击 **创建集群**。点击 **创建集群** 后会有以下动作：

   1. 如果您选择**「下载软件包」**，则会在节点中下载集群软件包。
   2. 在节点中安装集群软件（或者确认安装了正确的软件包）。
   3. 在集群的每个节点中更新并传推广群配置文件。
   4. 加入该集群的添加的节点

   显示的信息表示正在创建该集群。当集群准备好后，该显示会演示新创建集群的状态，请注意：如果没有在任何节点中运行 **ricci**，则该集群创建会失败。

   ![](https://blog-image.nos-eastchina1.126.net/luci-04.png)

5. 点击 **创建集群** 按钮创建集群后，您仍可以通过点击集群节点显示页面上部菜单中的**「添加」**或者**「删除」**功能从集群中添加或者删除节点。除非您要删除整个集群，否则必须在删除节点前停止它们。

   **注意**

   从集群中删除集群节点是一个破坏性操作，不能撤销。



## 配置Fence设备



### 创建 Fence 设备

要创建 fence 设备请按照以下步骤执行：

1. 在**「Fence 设备」**配置页面中，点击**「添加」**。点击**「添加」**显示**「添加 Fence 设备（事务）」**对话框。在这个对话框中选择要配置的 fence 设备类型。
2. 在**「添加 Fence 设备（事务）」**对话框中根据 fence 设备类型指定信息。
3. 点击 **提交**。

添加 fence 设备后，它会出现在**「Fence 设备」**配置页面中。



##  为集群成员配置 FENCING

完成创建集群和创建集群 fence 设备的初始步骤后，需要为集群节点配置 fencing。要在创建新集群后为节点配置 fencing 并为其配置 fence 设备，请按照本小节中的步骤执行。请注意：您必须为集群中的每个节点配置 fencing。

以下小节中提供了为节点配置单一 fence 设备、使用备份 fence 设备配置节点以及使用冗余电源配置节点的步骤：



### 为节点配置单一 Fence 设备

使用以下步骤配置有单一 fence 设备的节点。

1. 在具体集群页面中，您可以点击集群显示顶部的**「节点」**，为集群中的节点配置 fencing。这样做会显示组成集群的节点。当您点击 luci **「Homebase」**页面左侧菜单中的**「管理集群」**项下的集群名称时会出现这个默认页面。

2. 点击节点名称。点击节点链接会出现一个演示如何配置该节点的页面。

   在具体节点页面中显示所有目前在该节点中运行的服务，同时还显示该节点所在故障切换域。您可以点击其名称修改现有故障切换域。

3. 请在具体节点页面的**「Fence 设备」**项下点击 **添加 Fence 方法**。此时会显示 **在节点中添加 Fence 方法** 对话框。

4. 请输入为这个节点配置的 fencing 方法的**「方法名」**。这可以是红帽高可用性附加组件使用的任意名称。这与该设备的 DNS 名称不同。

5. 点击 **提交**。此时会显示具体节点页面，该页面中显示您刚刚在**「Fence 设备」**中添加的方法。

6. 点击 fence 事务下的 **添加 Fence 事务** 标签为这个方法配置 fence 事务。此时会出现**「添加 Fence 设备（事务）」**下拉菜单，可从中选择您之前配置的 fence 设备。

7. 为这个方法选择 fence 设备。如果这个 fence 设备需要您配置具体节点参数，则会显示要配置的参数。

   **注意**

   对于非电源 fence 方法（即 SAN/存储 fencing），会在具体节点参数显示中默认选择**「取消 fencing（Unfencing）」**。这可保证在重启该节点前不会重新启用被 fence 的节点对存储的访问。有关 unfencing 节点的详情请参考 `fence_node`(8) man page。

8. 点击 **提交**。此时会返回显示 fence 方法和 fence 事务的具体节点页面。



## 配置故障切换域

故障切换域是一个命名的集群节点子集，它可在节点失败事件中运行集群服务。故障切换域有以下特征：

- 无限制 — 允许您为在子集指定首选成员子集，但分配给这个域名的集群服务可在任意可用成员中运行。

- 限制 — 允许您限制可运行具体集群服务的成员。如果在限制故障切换域中没有可用成员，则无法启动集群服务（手动或者使用集群软件均不可行）。

- 无序 — 当将一个集群服务分配给一个无序故障切换域时，则可从可用故障切换域成员中随机选择运行集群服务的成员，没有优先顺序。

- 有序的 — 可让您在故障切换域的成员间指定顺序。该列表顶端的成员是首选成员，接下来是列表中的第二个成员，依此类推。

- 故障恢复 — 允许您指定在故障切换域中的服务是否应该恢复到节点失败前最初运行的节点。配置这个特性在作为有序故障切换域一部分节点重复失败的环境中很有帮助。在那种情况下，如果某个节点是故障切换域中的首选节点，在可能在首选节点和其它节点间重复切换和恢复某个服务，从而不会对性能产生严重影响。

  **注意**

  故障恢复特性只适用于配置了有序故障切换的集群。

**注意**

更改故障切换域配置对目前运行中的服务无效。

**注意**

操作*不需要*的故障切换域。

默认情况下故障切换域为无限制和无序的。

在由几个成员组成的集群中，使用限制故障切换域可最大程度降低设置集群以便运行集群服务的工作（比如 `httpd`），它要求您在运行该集群服务的所有成员中进行完全一致的配置。您不需要将整个集群设置为运行该集群服务，只要设置与该集群服务关联的限制故障切换域中的成员即可。

**注意**

要配置首选成员，您可以创建只有一个集群成员的无限制故障切换域。这样做就让集群服务主要在那个集群成员（首选成员）中运行，但允许将该集群服务故障切换到任意其它成员中。

### 添加故障切换域

要添加故障切换域，请按照本小节中的步骤执行。

1. 在具体集群页面中，您可以点击集群显示顶部的**「故障切换域」**为那个集群配置故障切换域。此时会显示为这个集群配置的故障切换域。

2. 点击**「添加」**。点击**「添加」**时会显示**「在集群中添加故障切换域」**。

   ![](https://blog-image.nos-eastchina1.126.net/failover-01.png)

   

3. 在**「在集群中添加故障切换域」**对话框的**「名称」**文本框中指定故障切换域名称。

   **注意**

   该名称应该可以与集群中其它名称所显示的目的区别。

4. 要启用在故障切换域成员间设置故障切换优先权，请点击**「优先的」**复选框。选择**「优先的」**复选框后，您可以为选择作为故障切换域成员的每个节点设置优先值，**「优先权」**。

5. 要限制这个故障切换域成员的故障切换，请点击**「有限」**复选框。选择**「有限」**复选框后，分配给这个故障切换域的服务只能切换到这个故障切换域中的节点。

6. 要将那个节点指定为不在这个故障切换域中恢复，请点击**「无故障恢复」**复选框。选择**「无故障恢复」**后，如果从首选节点中恢复某个服务，则该服务不会切换到恢复它的节点中。

7. 配置这个故障切换域的成员。为每个要成为故障切换域成员的节点点击**「成员」**复选框。如果选择**「优先的」**复选框，则请为故障切换域每个成员在**「优先权」**文本框中设置优先权。

8. 点击 **创建** 按钮。此时会显示新创建故障切换域的**「故障切换域」**页面。出现一条信息显示创建了新的域。刷新该页面查看更新的状态。

##  配置全局集群资源

您可以配置在集群中运行的任意服务所使用的全局资源，还可以配置只可用于具体服务的资源。

要添加全局集群资源，请按照本小节中的步骤操作。您可在配置该服务时，添加属于具体服务的本地资源

1. 在具体集群页面中，您可点击集群显示顶部的**「资源」**菜单在那个集群中添加资源。此时会显示已经为那个集群添加的资源。
2. 点击**「添加」**。此时会显示**「在集群中添加资源」**下拉菜单。
3. 点击**「在集群中添加资源」**中的下拉框并选择要配置的资源类型。
4. 输入您要添加资源的资源参数。
5. 点击 **提交**。点击 **提交** 按钮会返回显示**「资源」**信息的资源页面，此时该页面会显示添加的资源（和其它资源）。

要修改现有资源，请执行以下步骤。

1. 在 **luci** **「资源」**页面中点击要修改的资源名称。此时会显示那个资源的参数。
2. 编辑该资源的参数。
3. 点击 **应用**。

要删除现有资源，请执行以下步骤。

1. 在 **luci** **「资源」**页面中选择所有要删除资源。
2. 点击**「删除」**。





## 在集群中添加集群服务



1. 在具体集群页面中您可以点击集群显示顶部的**「服务组」**菜单在那个集群中添加服务。此时会显示已经为那个集群配置的服务。

2. 点击**「添加」**。此时会显示**「在集群中添加服务组」**对话框。

3. 在**「在集群中添加服务组」**对话框的**「服务名称」**文本框中输入该服务名称。

   **注意**

   请使用可明确与集群中的其它服务区别开来的描述性名称。

4. 如果您想在启动并运行集群时自动启动该服务，请选择**「自动启动这个服务」**复选框。如果*没有*选择这个复选框，则您必须在集群不处于停止状态时手动启动该服务。

5. 选择**「独家运行」**复选框设置策略，即该服务只在没有其它服务运行的节点中运行。

6. 如果您已经为该集群配置了故障切换域，您可以使用**「故障切换域」**参数的下拉菜单为该服务选择故障切换域。

7. 使用**「恢复策略」**下拉框为该服务选择恢复策略。选项包括 **「重新定位」**、**「重启」**、**「重启-禁用」** 或者 **「禁用」**该服务。

   选择**「重启」**选项表示在重新定位该服务前系统应尝试重启失败的服务。选择**「重新定位」**选项表示系统应在不同节点中重启该服务。选择**「禁用」**选项表示如果任意组件失败，系统就应禁用该资源组。选择**「重启-禁用」**选项表示该服务失败的位置尝试重启该服务，但如果重启失败，则将禁用服务而不是移动到集群的另一台主机中。

   如果您选择**「重启」**或者**「重启-禁用」**作为该服务的恢复策略，您可以指定重新定位或者禁用该服务前最多重启失败的次数，您还可以在多少秒后不再重启。

8. 要在服务中添加资源，请点击 **添加资源**。点击 **添加资源** 按钮会显示一个 **在服务中添加资源** 下拉菜单，您可从中选择要添加的现有全局资源，或者添加一个*只可*用于这个服务的新资源。

   - 要添加现有全局资源，请在 **在服务中添加资源** 下拉框中点击现有资源名称。此时会显示在您所配置服务的**「服务组」**页面中的资源及其参数。

   - 要添加只可用于这个服务的新资源，请在 **在服务中添加资源** 下拉框中选择要配置的资源类型并为您要添加的资源输入资源参数。

   - 当在服务中添加资源时，无论它是现有全局资源，还是只可用于这个服务的资源，您可将该资源指定为**「独立子树」**或者**「非关键资源」**。

     如果您将资源指定为独立子树，那么如果该资源失败，则在系统尝试常规恢复前只会重启那个资源（而不是整个服务）。您可以指定在该节点中为该服务使用恢复策略前最多尝试重启该资源的次数。您还可以指定在多少秒后系统将为该服务使用恢复策略。

     如果您将该资源指定为非关键资源，那么如果那个资源失败，则只需要重启该资源。同时如果该资源仍失败，那么只会禁用那个资源而不是整个服务。您可以指定在该节点中禁用该资源前最多重启该资源的次数。您还可以指定在多少秒后系统将禁用该资源。

9. 如果您要在您定义的资源中添加子资源，请点击 **添加子资源**。点击 **添加子资源** 后会显示**「在服务中添加资源」**下拉框，您可从中添加现有全局资源或者添加只可用于这个服务的新资源。您可以继续为这个资源添加子资源以适应您的要求。

   **注意**

   如果您要添加 Samba 服务资源，请将 Samba 服务资源直接连接到该服务，而*不是*服务中的资源。

10. 当您完成为该服务添加资源，并完成为资源添加子资源时，点击 **提交**。点击 **提交** 按钮后会返回显示添加的服务（以及其它服务）的**「服务组」**页面。

